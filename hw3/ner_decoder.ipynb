{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner-decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NULabTMN/ps3-Connor-Frazier/blob/dev/ner_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsY0LJOIT_cG",
        "colab_type": "text"
      },
      "source": [
        "# Implementing a Viterbi Decoder and Evaluation for Sequence Labeling\n",
        "\n",
        "In this assignment, you will build a Viterbi decoder for an LSTM named-entity recognition model. As we mentioned in class, recurrent and bidirectional recurrent neural networks, of which LSTMs are the most common examples, can be used to perform sequence labeling. Although these models encode information from the surrounding words in order to make predictions, there are no \"hard\" constraints on what tags can appear where.\n",
        "\n",
        "There hard constraints are particularly important for tasks that label spans of more than one token. The most common example of a span-labeling task is named-entity recognition (NER). As described in Eisenstein, Jurafksy & Martin, and other texts, the goal of NER is to label spans of one or more words as _mentions_ of an _entity_, such as a person, location, organization, etc.\n",
        "\n",
        "The most common approach to NER is to reduce it to a sequence-labeling task, where each token in the input is labeled either with an `O`, if it is \"outside\" any named-entity span, or with `B-TYPE`, if it is the first token in an entity of type `TYPE`, or with `I-TYPE`, if it is the second or later token in an entity of type `TYPE`. Distinguishing between the first and later tokens of an entity allow us to identify distinct entity spans even when they are adjacent.\n",
        "\n",
        "Common values of `TYPE` include `PER` for person, `LOC` for location, `DATE` for date, and so on. In the dataset we load below, there are 17 distinct types.\n",
        "\n",
        "The span-labeling scheme just described implies that the labels on tokens must obey certain constraints: the tag `I-PER` must follow either `B-PER` or another `I-PER`. I cannot follow `O`, `B-LOC`, or `I-LOC`, i.e., a tag for a different entity type. By themselves, LSTMs or bidirectional LSTMs cannot directly enforce these constraints. This is one reason why conditional random fields (CRFs), which _can_ enforce these constraints, are often layered on top of these recurrent models.\n",
        "\n",
        "In this assignment, you will implement the simplest possible CRF: a CRF so simple that it does not require any training. Rather, it will assign weight 1 to any sequence of tags that obeys the constraints and weight 0 to any sequence of tags that violates them. The inputs to the CRF, which are analogous to the emission probabilities in an HMM, will come from an LSTM.\n",
        "\n",
        "But first, in order to test your decoder, you will also implement some functions to evaluate the output of an NER system according to two metrics:\n",
        "1. You will count the number of _violations_ of the NER label constraints, i.e., how many times `I-TYPE` follows `O` or a tag of a different type. This number will be greater than 0 in the raw LSTM output, but should be 0 for your CRF output.\n",
        "1. You will compute the _span-level_ precision, recall, and F1 of NER output. Although the baseline LSTM was trained to achieve high _token-level_ accuracy, this metric can be misleadingly high, since so many tokens are correctly labeled `O`. In other words, what proportion of spans predicted by the model line up exactly with spans in the gold standard, and what proportion of spans in the gold standard were predicted by the model? For more, see the original task definition: https://www.aclweb.org/anthology/W03-0419/.\n",
        "\n",
        "We start with loading some code and data and the describe your tasks in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhnn49QEU_Ik",
        "colab_type": "text"
      },
      "source": [
        "## Set Up Dependencies and Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJINX1MwOLBT",
        "colab_type": "code",
        "outputId": "130ab6fa-7b1d-4648-cd0d-d22994781a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade spacy allennlp\n",
        "import spacy\n",
        "print(spacy.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 25.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.0.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 67.1MB/s \n",
            "\u001b[?25hCollecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 62.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.12.31)\n",
            "Requirement already satisfied, skipping upgrade: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/6b/fbb2d499b96861a18c1641f6fefe775110d3faba65c1524950e9ad64824a/jsonpickle-1.3-py2.py3-none-any.whl\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 65.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 11.0MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.1)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/b8/a8588d4010f13716a324f55d23999259bad9db2320f4fe919a66b2f651f3/jsonnet-0.15.0.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 57.3MB/s \n",
            "\u001b[?25hCollecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/52/8063322bd9ee6e7921b74fcb730c6ba983ff995ddfabd966bb689e313464/responses-0.10.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Collecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/ca/5b5962361ed832847b6b2f9a2d0452c8c2f29a93baef850bb8ad067c7bf9/gevent-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 46.2MB/s \n",
            "\u001b[?25hCollecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.11.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.15.31)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.6)\n",
            "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.3)\n",
            "Requirement already satisfied, skipping upgrade: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Building wheels for collected packages: numpydoc, ftfy, word2number, overrides, parsimonious, jsonnet\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31893 sha256=7d3ea465e1630a0415bd923e81d472642ef629cca3ee8183d92d7f7c68af73f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=98d8bf564cdf532d4b8f48da91243a79194a3d6241d30b9c3a17c465b3fca541\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=c6c716f8618c6c534ab0499831295d362f98829adfdd1aba4d8d813c58397ed3\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5609 sha256=dcf5dc209c27aecfcf2cf36f181d922d7be948bb68646a807102a7e5b13bf5ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42712 sha256=ee44a2eddc6fd12231f3304a487f781028556e05e9a438d836f9aecafd502b1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.15.0-cp36-cp36m-linux_x86_64.whl size=3319823 sha256=eccbd4c99ca45e320e902b38746fdeec6e6eaa1ce212d253f516f6889bf59005\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/63/2e/da89cfe1ba08550bd7262d5d9c027edc313980c3b85b3b0a38\n",
            "Successfully built numpydoc ftfy word2number overrides parsimonious jsonnet\n",
            "\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboardX, numpydoc, sentencepiece, pytorch-transformers, jsonpickle, pytorch-pretrained-bert, flask-cors, ftfy, word2number, overrides, parsimonious, jsonnet, unidecode, conllu, responses, greenlet, gevent, flaky, allennlp\n",
            "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.7 gevent-1.4.0 greenlet-0.4.15 jsonnet-0.15.0 jsonpickle-1.3 numpydoc-0.9.2 overrides-2.8.0 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.12 sentencepiece-0.1.85 tensorboardX-2.0 unidecode-1.1.1 word2number-1.1\n",
            "2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4zJfaIlJ2bv",
        "colab_type": "code",
        "outputId": "a71ac061-f24e-4836-c45c-564c51c5b8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from typing import Iterator, List, Dict\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.fields import TextField, SequenceLabelField\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.training.trainer import Trainer\n",
        "from allennlp.predictors import SentenceTaggerPredictor\n",
        "from allennlp.data.dataset_readers import conll2003\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdccd5de430>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo16Ko0Gchxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmTagger(Model):\n",
        "  def __init__(self,\n",
        "               word_embeddings: TextFieldEmbedder,\n",
        "               encoder: Seq2SeqEncoder,\n",
        "               vocab: Vocabulary) -> None:\n",
        "    super().__init__(vocab)\n",
        "    self.word_embeddings = word_embeddings\n",
        "    self.encoder = encoder\n",
        "    self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "    self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "  def forward(self,\n",
        "              tokens: Dict[str, torch.Tensor],\n",
        "              metadata,\n",
        "              tags: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
        "    mask = get_text_field_mask(tokens)\n",
        "    embeddings = self.word_embeddings(tokens)\n",
        "    encoder_out = self.encoder(embeddings, mask)\n",
        "    tag_logits = self.hidden2tag(encoder_out)\n",
        "    output = {\"tag_logits\": tag_logits}\n",
        "    if tags is not None:\n",
        "      self.accuracy(tag_logits, tags, mask)\n",
        "      output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, tags, mask)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "    return {\"accuracy\": self.accuracy.get_metric(reset)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdKvPftVVLt",
        "colab_type": "text"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sOVVZslKm3N",
        "colab_type": "code",
        "outputId": "64be132a-a5f9-4c42-9082-12f7348a2a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "reader = conll2003.Conll2003DatasetReader()\n",
        "train_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.train.ner.sample'))\n",
        "validation_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.development.ner.sample'))\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159377B [00:00, 748906.67B/s]\n",
            "562it [00:00, 5129.56it/s]\n",
            "8366B [00:00, 13301572.12B/s]\n",
            "23it [00:00, 4306.84it/s]\n",
            "100%|██████████| 585/585 [00:00<00:00, 48176.31it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpg2Udr-Vnwm",
        "colab_type": "text"
      },
      "source": [
        "## Define and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kDQQBMywdKx",
        "colab_type": "code",
        "outputId": "74231101-e537-474a-e244-0362b68035a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, bidirectional=False, batch_first=True))\n",
        "model = LstmTagger(word_embeddings, lstm, vocab)\n",
        "if torch.cuda.is_available():\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    cuda_device = -1\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)\n",
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset,\n",
        "                  patience=10,\n",
        "                  num_epochs=100,\n",
        "                  cuda_device=cuda_device)\n",
        "trainer.train()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8442, loss: 0.9097 ||: 100%|██████████| 281/281 [00:01<00:00, 176.65it/s]\n",
            "accuracy: 0.7878, loss: 1.1954 ||: 100%|██████████| 12/12 [00:00<00:00, 455.30it/s]\n",
            "accuracy: 0.8442, loss: 0.7293 ||: 100%|██████████| 281/281 [00:01<00:00, 257.78it/s]\n",
            "accuracy: 0.7878, loss: 1.2064 ||: 100%|██████████| 12/12 [00:00<00:00, 475.16it/s]\n",
            "accuracy: 0.8442, loss: 0.7160 ||: 100%|██████████| 281/281 [00:01<00:00, 252.62it/s]\n",
            "accuracy: 0.7878, loss: 1.1757 ||: 100%|██████████| 12/12 [00:00<00:00, 418.33it/s]\n",
            "accuracy: 0.8442, loss: 0.7072 ||: 100%|██████████| 281/281 [00:01<00:00, 261.38it/s]\n",
            "accuracy: 0.7878, loss: 1.1750 ||: 100%|██████████| 12/12 [00:00<00:00, 456.92it/s]\n",
            "accuracy: 0.8442, loss: 0.6984 ||: 100%|██████████| 281/281 [00:01<00:00, 259.26it/s]\n",
            "accuracy: 0.7878, loss: 1.1539 ||: 100%|██████████| 12/12 [00:00<00:00, 434.61it/s]\n",
            "accuracy: 0.8442, loss: 0.6914 ||: 100%|██████████| 281/281 [00:01<00:00, 259.17it/s]\n",
            "accuracy: 0.7878, loss: 1.1573 ||: 100%|██████████| 12/12 [00:00<00:00, 451.07it/s]\n",
            "accuracy: 0.8442, loss: 0.6817 ||: 100%|██████████| 281/281 [00:01<00:00, 251.94it/s]\n",
            "accuracy: 0.7878, loss: 1.1481 ||: 100%|██████████| 12/12 [00:00<00:00, 478.49it/s]\n",
            "accuracy: 0.8442, loss: 0.6710 ||: 100%|██████████| 281/281 [00:01<00:00, 250.79it/s]\n",
            "accuracy: 0.7878, loss: 1.1239 ||: 100%|██████████| 12/12 [00:00<00:00, 460.45it/s]\n",
            "accuracy: 0.8442, loss: 0.6564 ||: 100%|██████████| 281/281 [00:01<00:00, 260.20it/s]\n",
            "accuracy: 0.7878, loss: 1.1225 ||: 100%|██████████| 12/12 [00:00<00:00, 406.53it/s]\n",
            "accuracy: 0.8442, loss: 0.6396 ||: 100%|██████████| 281/281 [00:01<00:00, 258.15it/s]\n",
            "accuracy: 0.7878, loss: 1.0812 ||: 100%|██████████| 12/12 [00:00<00:00, 458.51it/s]\n",
            "accuracy: 0.8442, loss: 0.6153 ||: 100%|██████████| 281/281 [00:01<00:00, 255.98it/s]\n",
            "accuracy: 0.7878, loss: 1.0391 ||: 100%|██████████| 12/12 [00:00<00:00, 447.34it/s]\n",
            "accuracy: 0.8442, loss: 0.5804 ||: 100%|██████████| 281/281 [00:01<00:00, 257.18it/s]\n",
            "accuracy: 0.7878, loss: 0.9854 ||: 100%|██████████| 12/12 [00:00<00:00, 469.44it/s]\n",
            "accuracy: 0.8460, loss: 0.5408 ||: 100%|██████████| 281/281 [00:01<00:00, 259.20it/s]\n",
            "accuracy: 0.7878, loss: 0.9423 ||: 100%|██████████| 12/12 [00:00<00:00, 463.33it/s]\n",
            "accuracy: 0.8561, loss: 0.5038 ||: 100%|██████████| 281/281 [00:01<00:00, 255.41it/s]\n",
            "accuracy: 0.7878, loss: 0.8715 ||: 100%|██████████| 12/12 [00:00<00:00, 485.98it/s]\n",
            "accuracy: 0.8591, loss: 0.4727 ||: 100%|██████████| 281/281 [00:01<00:00, 255.84it/s]\n",
            "accuracy: 0.7878, loss: 0.8371 ||: 100%|██████████| 12/12 [00:00<00:00, 478.33it/s]\n",
            "accuracy: 0.8598, loss: 0.4481 ||: 100%|██████████| 281/281 [00:01<00:00, 244.73it/s]\n",
            "accuracy: 0.7918, loss: 0.8053 ||: 100%|██████████| 12/12 [00:00<00:00, 438.54it/s]\n",
            "accuracy: 0.8615, loss: 0.4302 ||: 100%|██████████| 281/281 [00:01<00:00, 260.86it/s]\n",
            "accuracy: 0.7939, loss: 0.7727 ||: 100%|██████████| 12/12 [00:00<00:00, 452.14it/s]\n",
            "accuracy: 0.8617, loss: 0.4165 ||: 100%|██████████| 281/281 [00:01<00:00, 261.30it/s]\n",
            "accuracy: 0.7939, loss: 0.7781 ||: 100%|██████████| 12/12 [00:00<00:00, 475.00it/s]\n",
            "accuracy: 0.8626, loss: 0.4058 ||: 100%|██████████| 281/281 [00:01<00:00, 258.48it/s]\n",
            "accuracy: 0.7878, loss: 0.7600 ||: 100%|██████████| 12/12 [00:00<00:00, 477.23it/s]\n",
            "accuracy: 0.8627, loss: 0.3990 ||: 100%|██████████| 281/281 [00:01<00:00, 258.22it/s]\n",
            "accuracy: 0.7939, loss: 0.7374 ||: 100%|██████████| 12/12 [00:00<00:00, 468.13it/s]\n",
            "accuracy: 0.8638, loss: 0.3925 ||: 100%|██████████| 281/281 [00:01<00:00, 257.90it/s]\n",
            "accuracy: 0.7939, loss: 0.7444 ||: 100%|██████████| 12/12 [00:00<00:00, 414.12it/s]\n",
            "accuracy: 0.8645, loss: 0.3851 ||: 100%|██████████| 281/281 [00:01<00:00, 257.77it/s]\n",
            "accuracy: 0.7939, loss: 0.7202 ||: 100%|██████████| 12/12 [00:00<00:00, 420.38it/s]\n",
            "accuracy: 0.8647, loss: 0.3802 ||: 100%|██████████| 281/281 [00:01<00:00, 258.49it/s]\n",
            "accuracy: 0.7939, loss: 0.7130 ||: 100%|██████████| 12/12 [00:00<00:00, 478.22it/s]\n",
            "accuracy: 0.8643, loss: 0.3759 ||: 100%|██████████| 281/281 [00:01<00:00, 264.45it/s]\n",
            "accuracy: 0.7939, loss: 0.7249 ||: 100%|██████████| 12/12 [00:00<00:00, 477.57it/s]\n",
            "accuracy: 0.8654, loss: 0.3732 ||: 100%|██████████| 281/281 [00:01<00:00, 258.15it/s]\n",
            "accuracy: 0.7939, loss: 0.7031 ||: 100%|██████████| 12/12 [00:00<00:00, 457.04it/s]\n",
            "accuracy: 0.8662, loss: 0.3708 ||: 100%|██████████| 281/281 [00:01<00:00, 260.94it/s]\n",
            "accuracy: 0.8020, loss: 0.7071 ||: 100%|██████████| 12/12 [00:00<00:00, 408.53it/s]\n",
            "accuracy: 0.8690, loss: 0.3641 ||: 100%|██████████| 281/281 [00:01<00:00, 246.49it/s]\n",
            "accuracy: 0.8000, loss: 0.6948 ||: 100%|██████████| 12/12 [00:00<00:00, 442.37it/s]\n",
            "accuracy: 0.8698, loss: 0.3623 ||: 100%|██████████| 281/281 [00:01<00:00, 246.61it/s]\n",
            "accuracy: 0.7939, loss: 0.6907 ||: 100%|██████████| 12/12 [00:00<00:00, 444.18it/s]\n",
            "accuracy: 0.8699, loss: 0.3581 ||: 100%|██████████| 281/281 [00:01<00:00, 257.23it/s]\n",
            "accuracy: 0.7980, loss: 0.6895 ||: 100%|██████████| 12/12 [00:00<00:00, 456.73it/s]\n",
            "accuracy: 0.8719, loss: 0.3539 ||: 100%|██████████| 281/281 [00:01<00:00, 258.34it/s]\n",
            "accuracy: 0.8020, loss: 0.6843 ||: 100%|██████████| 12/12 [00:00<00:00, 465.99it/s]\n",
            "accuracy: 0.8720, loss: 0.3509 ||: 100%|██████████| 281/281 [00:01<00:00, 254.47it/s]\n",
            "accuracy: 0.7980, loss: 0.6954 ||: 100%|██████████| 12/12 [00:00<00:00, 476.26it/s]\n",
            "accuracy: 0.8746, loss: 0.3472 ||: 100%|██████████| 281/281 [00:01<00:00, 259.18it/s]\n",
            "accuracy: 0.8041, loss: 0.6674 ||: 100%|██████████| 12/12 [00:00<00:00, 461.22it/s]\n",
            "accuracy: 0.8730, loss: 0.3464 ||: 100%|██████████| 281/281 [00:01<00:00, 252.76it/s]\n",
            "accuracy: 0.8020, loss: 0.6575 ||: 100%|██████████| 12/12 [00:00<00:00, 478.83it/s]\n",
            "accuracy: 0.8747, loss: 0.3404 ||: 100%|██████████| 281/281 [00:01<00:00, 257.58it/s]\n",
            "accuracy: 0.8082, loss: 0.6585 ||: 100%|██████████| 12/12 [00:00<00:00, 476.83it/s]\n",
            "accuracy: 0.8746, loss: 0.3383 ||: 100%|██████████| 281/281 [00:01<00:00, 252.61it/s]\n",
            "accuracy: 0.8204, loss: 0.6592 ||: 100%|██████████| 12/12 [00:00<00:00, 447.89it/s]\n",
            "accuracy: 0.8774, loss: 0.3351 ||: 100%|██████████| 281/281 [00:01<00:00, 260.00it/s]\n",
            "accuracy: 0.8082, loss: 0.6671 ||: 100%|██████████| 12/12 [00:00<00:00, 462.51it/s]\n",
            "accuracy: 0.8774, loss: 0.3338 ||: 100%|██████████| 281/281 [00:01<00:00, 262.14it/s]\n",
            "accuracy: 0.8143, loss: 0.6391 ||: 100%|██████████| 12/12 [00:00<00:00, 450.96it/s]\n",
            "accuracy: 0.8780, loss: 0.3306 ||: 100%|██████████| 281/281 [00:01<00:00, 258.42it/s]\n",
            "accuracy: 0.8204, loss: 0.6630 ||: 100%|██████████| 12/12 [00:00<00:00, 479.07it/s]\n",
            "accuracy: 0.8795, loss: 0.3276 ||: 100%|██████████| 281/281 [00:01<00:00, 259.77it/s]\n",
            "accuracy: 0.8224, loss: 0.6449 ||: 100%|██████████| 12/12 [00:00<00:00, 479.63it/s]\n",
            "accuracy: 0.8823, loss: 0.3249 ||: 100%|██████████| 281/281 [00:01<00:00, 262.55it/s]\n",
            "accuracy: 0.8265, loss: 0.6367 ||: 100%|██████████| 12/12 [00:00<00:00, 479.31it/s]\n",
            "accuracy: 0.8830, loss: 0.3211 ||: 100%|██████████| 281/281 [00:01<00:00, 260.00it/s]\n",
            "accuracy: 0.8286, loss: 0.6297 ||: 100%|██████████| 12/12 [00:00<00:00, 481.17it/s]\n",
            "accuracy: 0.8814, loss: 0.3237 ||: 100%|██████████| 281/281 [00:01<00:00, 254.62it/s]\n",
            "accuracy: 0.8347, loss: 0.6158 ||: 100%|██████████| 12/12 [00:00<00:00, 448.52it/s]\n",
            "accuracy: 0.8818, loss: 0.3210 ||: 100%|██████████| 281/281 [00:01<00:00, 256.57it/s]\n",
            "accuracy: 0.8347, loss: 0.6043 ||: 100%|██████████| 12/12 [00:00<00:00, 447.30it/s]\n",
            "accuracy: 0.8857, loss: 0.3173 ||: 100%|██████████| 281/281 [00:01<00:00, 259.92it/s]\n",
            "accuracy: 0.8327, loss: 0.6202 ||: 100%|██████████| 12/12 [00:00<00:00, 485.20it/s]\n",
            "accuracy: 0.8866, loss: 0.3153 ||: 100%|██████████| 281/281 [00:01<00:00, 258.53it/s]\n",
            "accuracy: 0.8347, loss: 0.6130 ||: 100%|██████████| 12/12 [00:00<00:00, 430.28it/s]\n",
            "accuracy: 0.8878, loss: 0.3111 ||: 100%|██████████| 281/281 [00:01<00:00, 258.19it/s]\n",
            "accuracy: 0.8306, loss: 0.6317 ||: 100%|██████████| 12/12 [00:00<00:00, 468.74it/s]\n",
            "accuracy: 0.8875, loss: 0.3105 ||: 100%|██████████| 281/281 [00:01<00:00, 256.62it/s]\n",
            "accuracy: 0.8286, loss: 0.6045 ||: 100%|██████████| 12/12 [00:00<00:00, 461.82it/s]\n",
            "accuracy: 0.8893, loss: 0.3086 ||: 100%|██████████| 281/281 [00:01<00:00, 260.14it/s]\n",
            "accuracy: 0.8347, loss: 0.6032 ||: 100%|██████████| 12/12 [00:00<00:00, 431.17it/s]\n",
            "accuracy: 0.8903, loss: 0.3055 ||: 100%|██████████| 281/281 [00:01<00:00, 258.24it/s]\n",
            "accuracy: 0.8408, loss: 0.5851 ||: 100%|██████████| 12/12 [00:00<00:00, 483.29it/s]\n",
            "accuracy: 0.8899, loss: 0.3040 ||: 100%|██████████| 281/281 [00:01<00:00, 258.44it/s]\n",
            "accuracy: 0.8490, loss: 0.5862 ||: 100%|██████████| 12/12 [00:00<00:00, 457.23it/s]\n",
            "accuracy: 0.8907, loss: 0.3033 ||: 100%|██████████| 281/281 [00:01<00:00, 258.82it/s]\n",
            "accuracy: 0.8367, loss: 0.5973 ||: 100%|██████████| 12/12 [00:00<00:00, 407.03it/s]\n",
            "accuracy: 0.8928, loss: 0.2995 ||: 100%|██████████| 281/281 [00:01<00:00, 252.51it/s]\n",
            "accuracy: 0.8449, loss: 0.5803 ||: 100%|██████████| 12/12 [00:00<00:00, 458.02it/s]\n",
            "accuracy: 0.8926, loss: 0.2965 ||: 100%|██████████| 281/281 [00:01<00:00, 262.37it/s]\n",
            "accuracy: 0.8490, loss: 0.5691 ||: 100%|██████████| 12/12 [00:00<00:00, 485.98it/s]\n",
            "accuracy: 0.8915, loss: 0.2972 ||: 100%|██████████| 281/281 [00:01<00:00, 258.60it/s]\n",
            "accuracy: 0.8408, loss: 0.5855 ||: 100%|██████████| 12/12 [00:00<00:00, 381.51it/s]\n",
            "accuracy: 0.8932, loss: 0.2944 ||: 100%|██████████| 281/281 [00:01<00:00, 261.73it/s]\n",
            "accuracy: 0.8490, loss: 0.5764 ||: 100%|██████████| 12/12 [00:00<00:00, 482.73it/s]\n",
            "accuracy: 0.8931, loss: 0.2924 ||: 100%|██████████| 281/281 [00:01<00:00, 256.17it/s]\n",
            "accuracy: 0.8408, loss: 0.5697 ||: 100%|██████████| 12/12 [00:00<00:00, 436.91it/s]\n",
            "accuracy: 0.8933, loss: 0.2907 ||: 100%|██████████| 281/281 [00:01<00:00, 259.14it/s]\n",
            "accuracy: 0.8449, loss: 0.5618 ||: 100%|██████████| 12/12 [00:00<00:00, 472.02it/s]\n",
            "accuracy: 0.8941, loss: 0.2888 ||: 100%|██████████| 281/281 [00:01<00:00, 255.06it/s]\n",
            "accuracy: 0.8531, loss: 0.5528 ||: 100%|██████████| 12/12 [00:00<00:00, 481.90it/s]\n",
            "accuracy: 0.8940, loss: 0.2856 ||: 100%|██████████| 281/281 [00:01<00:00, 257.43it/s]\n",
            "accuracy: 0.8449, loss: 0.5613 ||: 100%|██████████| 12/12 [00:00<00:00, 422.10it/s]\n",
            "accuracy: 0.8945, loss: 0.2836 ||: 100%|██████████| 281/281 [00:01<00:00, 246.76it/s]\n",
            "accuracy: 0.8408, loss: 0.5534 ||: 100%|██████████| 12/12 [00:00<00:00, 423.24it/s]\n",
            "accuracy: 0.8961, loss: 0.2819 ||: 100%|██████████| 281/281 [00:01<00:00, 261.04it/s]\n",
            "accuracy: 0.8510, loss: 0.5462 ||: 100%|██████████| 12/12 [00:00<00:00, 483.90it/s]\n",
            "accuracy: 0.8960, loss: 0.2807 ||: 100%|██████████| 281/281 [00:01<00:00, 257.72it/s]\n",
            "accuracy: 0.8531, loss: 0.5540 ||: 100%|██████████| 12/12 [00:00<00:00, 453.34it/s]\n",
            "accuracy: 0.8975, loss: 0.2792 ||: 100%|██████████| 281/281 [00:01<00:00, 256.68it/s]\n",
            "accuracy: 0.8510, loss: 0.5343 ||: 100%|██████████| 12/12 [00:00<00:00, 442.19it/s]\n",
            "accuracy: 0.8986, loss: 0.2780 ||: 100%|██████████| 281/281 [00:01<00:00, 258.95it/s]\n",
            "accuracy: 0.8490, loss: 0.5354 ||: 100%|██████████| 12/12 [00:00<00:00, 481.11it/s]\n",
            "accuracy: 0.9000, loss: 0.2732 ||: 100%|██████████| 281/281 [00:01<00:00, 259.64it/s]\n",
            "accuracy: 0.8490, loss: 0.5279 ||: 100%|██████████| 12/12 [00:00<00:00, 459.32it/s]\n",
            "accuracy: 0.9005, loss: 0.2717 ||: 100%|██████████| 281/281 [00:01<00:00, 264.75it/s]\n",
            "accuracy: 0.8510, loss: 0.5308 ||: 100%|██████████| 12/12 [00:00<00:00, 457.03it/s]\n",
            "accuracy: 0.9011, loss: 0.2696 ||: 100%|██████████| 281/281 [00:01<00:00, 253.42it/s]\n",
            "accuracy: 0.8510, loss: 0.5251 ||: 100%|██████████| 12/12 [00:00<00:00, 474.60it/s]\n",
            "accuracy: 0.9026, loss: 0.2677 ||: 100%|██████████| 281/281 [00:01<00:00, 255.14it/s]\n",
            "accuracy: 0.8551, loss: 0.5320 ||: 100%|██████████| 12/12 [00:00<00:00, 429.02it/s]\n",
            "accuracy: 0.9038, loss: 0.2641 ||: 100%|██████████| 281/281 [00:01<00:00, 253.56it/s]\n",
            "accuracy: 0.8531, loss: 0.5179 ||: 100%|██████████| 12/12 [00:00<00:00, 461.70it/s]\n",
            "accuracy: 0.9040, loss: 0.2641 ||: 100%|██████████| 281/281 [00:01<00:00, 261.30it/s]\n",
            "accuracy: 0.8592, loss: 0.5256 ||: 100%|██████████| 12/12 [00:00<00:00, 423.88it/s]\n",
            "accuracy: 0.9051, loss: 0.2607 ||: 100%|██████████| 281/281 [00:01<00:00, 255.68it/s]\n",
            "accuracy: 0.8592, loss: 0.5060 ||: 100%|██████████| 12/12 [00:00<00:00, 471.91it/s]\n",
            "accuracy: 0.9063, loss: 0.2581 ||: 100%|██████████| 281/281 [00:01<00:00, 261.15it/s]\n",
            "accuracy: 0.8551, loss: 0.5009 ||: 100%|██████████| 12/12 [00:00<00:00, 486.15it/s]\n",
            "accuracy: 0.9052, loss: 0.2577 ||: 100%|██████████| 281/281 [00:01<00:00, 255.88it/s]\n",
            "accuracy: 0.8612, loss: 0.4984 ||: 100%|██████████| 12/12 [00:00<00:00, 470.72it/s]\n",
            "accuracy: 0.9055, loss: 0.2556 ||: 100%|██████████| 281/281 [00:01<00:00, 255.52it/s]\n",
            "accuracy: 0.8531, loss: 0.4970 ||: 100%|██████████| 12/12 [00:00<00:00, 469.95it/s]\n",
            "accuracy: 0.9072, loss: 0.2541 ||: 100%|██████████| 281/281 [00:01<00:00, 260.66it/s]\n",
            "accuracy: 0.8531, loss: 0.4939 ||: 100%|██████████| 12/12 [00:00<00:00, 385.89it/s]\n",
            "accuracy: 0.9073, loss: 0.2503 ||: 100%|██████████| 281/281 [00:01<00:00, 258.29it/s]\n",
            "accuracy: 0.8612, loss: 0.4883 ||: 100%|██████████| 12/12 [00:00<00:00, 443.88it/s]\n",
            "accuracy: 0.9089, loss: 0.2484 ||: 100%|██████████| 281/281 [00:01<00:00, 255.14it/s]\n",
            "accuracy: 0.8551, loss: 0.4926 ||: 100%|██████████| 12/12 [00:00<00:00, 423.65it/s]\n",
            "accuracy: 0.9088, loss: 0.2462 ||: 100%|██████████| 281/281 [00:01<00:00, 245.83it/s]\n",
            "accuracy: 0.8571, loss: 0.4802 ||: 100%|██████████| 12/12 [00:00<00:00, 449.51it/s]\n",
            "accuracy: 0.9094, loss: 0.2445 ||: 100%|██████████| 281/281 [00:01<00:00, 256.38it/s]\n",
            "accuracy: 0.8551, loss: 0.4887 ||: 100%|██████████| 12/12 [00:00<00:00, 465.08it/s]\n",
            "accuracy: 0.9102, loss: 0.2418 ||: 100%|██████████| 281/281 [00:01<00:00, 253.21it/s]\n",
            "accuracy: 0.8612, loss: 0.4742 ||: 100%|██████████| 12/12 [00:00<00:00, 462.71it/s]\n",
            "accuracy: 0.9113, loss: 0.2401 ||: 100%|██████████| 281/281 [00:01<00:00, 258.45it/s]\n",
            "accuracy: 0.8531, loss: 0.4743 ||: 100%|██████████| 12/12 [00:00<00:00, 309.64it/s]\n",
            "accuracy: 0.9123, loss: 0.2365 ||: 100%|██████████| 281/281 [00:01<00:00, 260.38it/s]\n",
            "accuracy: 0.8612, loss: 0.4672 ||: 100%|██████████| 12/12 [00:00<00:00, 478.34it/s]\n",
            "accuracy: 0.9125, loss: 0.2350 ||: 100%|██████████| 281/281 [00:01<00:00, 255.37it/s]\n",
            "accuracy: 0.8633, loss: 0.4793 ||: 100%|██████████| 12/12 [00:00<00:00, 464.18it/s]\n",
            "accuracy: 0.9119, loss: 0.2327 ||: 100%|██████████| 281/281 [00:01<00:00, 249.92it/s]\n",
            "accuracy: 0.8633, loss: 0.4570 ||: 100%|██████████| 12/12 [00:00<00:00, 382.30it/s]\n",
            "accuracy: 0.9133, loss: 0.2304 ||: 100%|██████████| 281/281 [00:01<00:00, 256.38it/s]\n",
            "accuracy: 0.8612, loss: 0.4617 ||: 100%|██████████| 12/12 [00:00<00:00, 439.10it/s]\n",
            "accuracy: 0.9154, loss: 0.2283 ||: 100%|██████████| 281/281 [00:01<00:00, 251.02it/s]\n",
            "accuracy: 0.8592, loss: 0.4515 ||: 100%|██████████| 12/12 [00:00<00:00, 475.07it/s]\n",
            "accuracy: 0.9129, loss: 0.2257 ||: 100%|██████████| 281/281 [00:01<00:00, 256.78it/s]\n",
            "accuracy: 0.8633, loss: 0.4481 ||: 100%|██████████| 12/12 [00:00<00:00, 484.84it/s]\n",
            "accuracy: 0.9159, loss: 0.2256 ||: 100%|██████████| 281/281 [00:01<00:00, 262.74it/s]\n",
            "accuracy: 0.8633, loss: 0.4406 ||: 100%|██████████| 12/12 [00:00<00:00, 438.02it/s]\n",
            "accuracy: 0.9160, loss: 0.2215 ||: 100%|██████████| 281/281 [00:01<00:00, 257.19it/s]\n",
            "accuracy: 0.8612, loss: 0.4402 ||: 100%|██████████| 12/12 [00:00<00:00, 465.33it/s]\n",
            "accuracy: 0.9161, loss: 0.2205 ||: 100%|██████████| 281/281 [00:01<00:00, 257.13it/s]\n",
            "accuracy: 0.8653, loss: 0.4365 ||: 100%|██████████| 12/12 [00:00<00:00, 479.84it/s]\n",
            "accuracy: 0.9165, loss: 0.2183 ||: 100%|██████████| 281/281 [00:01<00:00, 254.67it/s]\n",
            "accuracy: 0.8633, loss: 0.4375 ||: 100%|██████████| 12/12 [00:00<00:00, 459.74it/s]\n",
            "accuracy: 0.9172, loss: 0.2168 ||: 100%|██████████| 281/281 [00:01<00:00, 263.56it/s]\n",
            "accuracy: 0.8633, loss: 0.4314 ||: 100%|██████████| 12/12 [00:00<00:00, 443.19it/s]\n",
            "accuracy: 0.9176, loss: 0.2150 ||: 100%|██████████| 281/281 [00:01<00:00, 255.65it/s]\n",
            "accuracy: 0.8612, loss: 0.4391 ||: 100%|██████████| 12/12 [00:00<00:00, 481.16it/s]\n",
            "accuracy: 0.9178, loss: 0.2129 ||: 100%|██████████| 281/281 [00:01<00:00, 259.25it/s]\n",
            "accuracy: 0.8714, loss: 0.4207 ||: 100%|██████████| 12/12 [00:00<00:00, 378.91it/s]\n",
            "accuracy: 0.9185, loss: 0.2111 ||: 100%|██████████| 281/281 [00:01<00:00, 257.47it/s]\n",
            "accuracy: 0.8653, loss: 0.4195 ||: 100%|██████████| 12/12 [00:00<00:00, 455.40it/s]\n",
            "accuracy: 0.9183, loss: 0.2086 ||: 100%|██████████| 281/281 [00:01<00:00, 254.56it/s]\n",
            "accuracy: 0.8653, loss: 0.4173 ||: 100%|██████████| 12/12 [00:00<00:00, 476.31it/s]\n",
            "accuracy: 0.9185, loss: 0.2093 ||: 100%|██████████| 281/281 [00:01<00:00, 255.61it/s]\n",
            "accuracy: 0.8673, loss: 0.4139 ||: 100%|██████████| 12/12 [00:00<00:00, 443.22it/s]\n",
            "accuracy: 0.9231, loss: 0.2058 ||: 100%|██████████| 281/281 [00:01<00:00, 258.37it/s]\n",
            "accuracy: 0.8694, loss: 0.4167 ||: 100%|██████████| 12/12 [00:00<00:00, 477.56it/s]\n",
            "accuracy: 0.9220, loss: 0.2036 ||: 100%|██████████| 281/281 [00:01<00:00, 261.29it/s]\n",
            "accuracy: 0.8694, loss: 0.4002 ||: 100%|██████████| 12/12 [00:00<00:00, 459.14it/s]\n",
            "accuracy: 0.9232, loss: 0.2016 ||: 100%|██████████| 281/281 [00:01<00:00, 259.91it/s]\n",
            "accuracy: 0.8673, loss: 0.4111 ||: 100%|██████████| 12/12 [00:00<00:00, 459.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 98,\n",
              " 'best_validation_accuracy': 0.8693877551020408,\n",
              " 'best_validation_loss': 0.4001997278537601,\n",
              " 'epoch': 99,\n",
              " 'peak_cpu_memory_MB': 2577.98,\n",
              " 'peak_gpu_0_memory_MB': 541,\n",
              " 'training_accuracy': 0.9232072873636268,\n",
              " 'training_cpu_memory_MB': 2577.98,\n",
              " 'training_duration': '0:01:58.214610',\n",
              " 'training_epochs': 99,\n",
              " 'training_gpu_0_memory_MB': 541,\n",
              " 'training_loss': 0.2016096050325728,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 0.8673469387755102,\n",
              " 'validation_loss': 0.41113841835370596}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwN6ctqVV0tf",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkDs_UdIeuFz",
        "colab_type": "text"
      },
      "source": [
        "The simple code below creators a `predictor` object that applies the model to an input example and then loops over the examples in the validation set, printing out the input token, gold-standard output, and model output. You can see from these methods how to access data and model outputs for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0bE4fmLik08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
        "\n",
        "def tag_sentence(s):\n",
        "  tag_ids = np.argmax(predictor.predict_instance(s)['tag_logits'], axis=-1)\n",
        "  fields = zip(s['tokens'], s['tags'], [model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])\n",
        "  return list(fields)\n",
        "\n",
        "baseline_output = [tag_sentence(i) for i in validation_dataset]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpYMx7RVfCyT",
        "colab_type": "text"
      },
      "source": [
        "Now, you can implement two evaluation functions: `violations` and `span_stats`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P042A2Ofg3wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: count the number of NER label violations,\n",
        "# such as O followed by I-TYPE or B-TYPE followed by\n",
        "# I-OTHER_TYPE\n",
        "# I-type can not be followed by another I-type\n",
        "\n",
        "# Valid moves:\n",
        "# From O, can go to O or B-type\n",
        "# From B-type can go to O or same I-type only or other B-type\n",
        "# From I-type can go to same I-type or 0\n",
        "\n",
        "# Invalid Move edge cases\n",
        "# I-type can not be first tag\n",
        "\n",
        "# Take tagger output as input\n",
        "def violations(tagged):\n",
        "  # Initialize violations count\n",
        "  count = 0\n",
        "  # Loop trhugh each sentence\n",
        "  for sentence in tagged:\n",
        "    # Loop through each word in the sentence up to the last word\n",
        "    for i in range(len(sentence) - 1):\n",
        "      # Compare the current ith word to the next word, add 1 to the violations count if there is one\n",
        "\n",
        "      # If the first word in the sentence gets an I-TYPE tag, there is a violation\n",
        "      if i == 0 and sentence[i][2] == 'I':\n",
        "        count += 1\n",
        "\n",
        "      # If the the next word after an O tag gets an I-TYPE tag, there is a violation\n",
        "      if sentence[i][2] == 'O' and sentence[i+1][2][0] == 'I':\n",
        "        count += 1\n",
        "\n",
        "      # If the next word after a B-TYPE tag gets an I-TYPE tag of a different type, there is a violation\n",
        "      if sentence[i][2][0] == 'B' and sentence[i + 1][2][0] == 'I' and sentence[i][2][2:] != sentence[i + 1][2][2:]:\n",
        "        count += 1\n",
        "\n",
        "      # If the next word after an I-TYPE tag gets an I-TYPE tag of a different tyoe, there is a violation\n",
        "      if sentence[i][2][0] == 'I' and sentence[i + 1][2] == 'I' and sentence[i][2][2:] != sentence[i + 1][2][2:]:\n",
        "        count += 1\n",
        "\n",
        "  return count\n",
        "\n",
        "# TODO: return the span-level precision, recall, and F1\n",
        "# Take tagger output as input\n",
        "def span_stats(tagged):\n",
        "\n",
        "  # Definitions\n",
        "  # true  postive = correct span that matches indexes as well\n",
        "  # recall = true positive / true positve + false negative  = # of correct matched spans  / true number of spans\n",
        "  # precision = true positive / true positve + false positive  = # of correct matched spans / number of spans guessed\n",
        "  # f measure 2rp / (r+ p)\n",
        "\n",
        "  # Find all the true spans with type and indices\n",
        "  true_spans = []\n",
        "  # Loop through all the sentences\n",
        "  for sentence in tagged:\n",
        "    # Initialie variables for finding spans\n",
        "    start_span = False\n",
        "    start_index = 0\n",
        "    end_index = 0\n",
        "    span_type = \"\"\n",
        "    # Loop through each word\n",
        "    for word in sentence:\n",
        "      # Start the span tracking\n",
        "      if word[1][0] == 'B' and start_span == False:\n",
        "        start_span = True\n",
        "        start_index = sentence.index(word)\n",
        "        span_type = word[1][2:]\n",
        "        # End the span tracking\n",
        "      if (word[1][0] == 'O' or word[1][0] == 'B') and start_span == True:\n",
        "        start_span = False\n",
        "        end_index = sentence.index(word) - 1\n",
        "        true_spans.append((span_type, start_index, end_index))\n",
        "        # If span ended with a new one, restart\n",
        "        if word[1][0] == 'B':\n",
        "          start_span = True\n",
        "          start_index = sentence.index(word)\n",
        "          span_type = word[1][2:]\n",
        "\n",
        "  # Find all the guessed spans with type and indices\n",
        "  guessed_spans = []\n",
        "  # Loop through all the sentences\n",
        "  for sentence in tagged:\n",
        "    # Initialie variables for finding spans\n",
        "    start_span = False\n",
        "    start_index = 0\n",
        "    end_index = 0\n",
        "    span_type = \"\"\n",
        "    # Loop through each word\n",
        "    for word in sentence:\n",
        "      # Start the span tracking\n",
        "      if word[2][0] == 'B' and start_span == False:\n",
        "        start_span = True\n",
        "        start_index = sentence.index(word)\n",
        "        span_type = word[2][2:]\n",
        "        # End the span tracking\n",
        "      if (word[2][0] == 'O' or word[2][0] == 'B') and start_span == True:\n",
        "        start_span = False\n",
        "        end_index = sentence.index(word) - 1\n",
        "        # If span ended with a new one, restart\n",
        "        guessed_spans.append((span_type, start_index, end_index))\n",
        "        if word[2][0] == 'B':\n",
        "          start_span = True\n",
        "          start_index = sentence.index(word)\n",
        "          span_type = word[2][2:]\n",
        "  \n",
        "  #Find the matching spans between the true spans and the guessed spans\n",
        "  spans_matched_count = 0  \n",
        "  for span in true_spans:\n",
        "    for guessed_span in guessed_spans:\n",
        "      if span[0] == guessed_span[0] and span[1] == guessed_span[1] and span[2] == guessed_span[2]:\n",
        "        spans_matched_count += 1\n",
        "\n",
        "  #Calculate Metrics    \n",
        "  recall = spans_matched_count/len(true_spans)\n",
        "  precision = spans_matched_count/len(guessed_spans)\n",
        "  f1 = 2*recall*precision / (recall + precision)        \n",
        "\n",
        "  return {'precision': precision,\n",
        "          'recall': recall,\n",
        "          'f1': f1}\n",
        "\n",
        "## You can check how many violations are made by the model output in predictor.\n",
        "# print(violations(baseline_output))\n",
        "# print(span_stats(baseline_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX7-quD2hnzB",
        "colab_type": "text"
      },
      "source": [
        "## Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCgW9d9ohsGv",
        "colab_type": "text"
      },
      "source": [
        "Now you can finally implement the simple Viterbi decoder. The `predictor` object, when applied to an input sentence, first calculates the scores for each possible output tag for each token. See the line `predictor.predict_instance(i)['tag_logits']` in the code above.\n",
        "\n",
        "Then, you will construct a transition matrix. You can use the code below to get a list of the tags the model knows about. For a set of K tags, construct a K-by-K matrix with a log(1)=0 when a transition between a given tag pair is valid and a log(0)=-infinity otherwise.\n",
        "\n",
        "Finally, implement a Viterbi decoder that takes the predictor object and a dataset object and outputs tagged data, just like the `tag_sentence` function above. It should use the Viterbi algorithm with the (max, plus) semiring. You'll be working with sums of log probabilities instead of products of probabilties.\n",
        "\n",
        "Run your `violations` function on the output of this decoder to make sure that there are no invalid tag transitions. Also, compare the span-level metrics on `baseline_output` and your new output using your `span_stats` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlP8vZhiS68K",
        "colab_type": "code",
        "outputId": "29423cce-efc7-4ffc-b4d7-ec5ab506e19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# This code show how to map from output vector components to labels\n",
        "# print(vocab.get_index_to_token_vocabulary('labels'))\n",
        "#Get tags\n",
        "tags = vocab.get_index_to_token_vocabulary('labels')\n",
        "\n",
        "#valid moves:\n",
        "#From O, can go to O or B-TYPE\n",
        "#From B-type can go to O or I-type only or to B-type\n",
        "#From I-type can go to same I-TYPE or 0\n",
        "\n",
        "#K-by-K matrix with a log(1)=0 when a transition between a given tag pair is valid and a log(0)=-infinity otherwise.\n",
        "\n",
        "#Create transition matrix, all transitions start as invalid\n",
        "transitions = np.zeros( (len(tags), len(tags)) )\n",
        "for r in range(len(tags)):\n",
        "  for c in range(len(tags)):\n",
        "    transitions[r][c] = np.log2(0)\n",
        "\n",
        "#Change matrix values for valid moves\n",
        "#Oriented so transition is from row tag to column tag\n",
        "#Rows and columns represents all tags\n",
        "for r in range(len(tags)):\n",
        "  for c in range(len(tags)):\n",
        "    # O tag to other tags\n",
        "    if tags[r] == 'O':\n",
        "      if tags[c] == 'O':\n",
        "        transitions[r][c] = np.log2(1)\n",
        "      if tags[c][0] == 'B':\n",
        "          transitions[r][c] = np.log2(1)\n",
        "\n",
        "    # B-TYPE to other tags\n",
        "    if tags[r][0] == 'B':\n",
        "      if tags[c][0] == 'I':\n",
        "          if tags[r][2:] == tags[c][2:]:\n",
        "            transitions[r][c] = np.log2(1)        \n",
        "      if tags[c] == 'O':\n",
        "        transitions[r][c] = np.log2(1)\n",
        "      if tags[c][0] == 'B':\n",
        "        transitions[r][c] = np.log2(1)\n",
        "\n",
        "    # I-TYPE to other tags\n",
        "    if tags[r][0] == 'I':\n",
        "      if tags[c][0] == 'I':\n",
        "        if tags[r][2:] == tags[c][2:]:\n",
        "          transitions[r][c] = np.log2(1)\n",
        "      if tags[c] == 'O':\n",
        "        transitions[r][c] = np.log2(1)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log2\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzMCIuIlyy1c",
        "colab_type": "code",
        "outputId": "362ddf38-71d4-4c5e-fd88-2bf1500edc18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Get the predictor object\n",
        "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
        "\n",
        "# Tag the sentence with the viterbi decoder\n",
        "def viterbi_tag_sentence(s):\n",
        "\n",
        "  # List of the correct tokens in the sentence\n",
        "  tokens = list(s['tokens'])\n",
        "\n",
        "  # List of lists that holds the scores of all tags across all tokens\n",
        "  word_tag_probabilities = predictor.predict_instance(s)['tag_logits'] \n",
        "\n",
        "  #Matrices used to hold scores of nodes and associated previous node\n",
        "  #rows are tags, columns are the words\n",
        "  sentence_scores = np.zeros((len(tags), len(tokens)))\n",
        "  sentence_tags = np.zeros((len(tags), len(tokens))) #place to hold preious tags??\n",
        "\n",
        "  # Set the first word tag nodes to their initial probabiliites\n",
        "  for i in range(len(tags)):\n",
        "    sentence_scores[i][0] = word_tag_probabilities[0][i]\n",
        "\n",
        "  # Loop through each token from 2 to n\n",
        "  for k in range(len(tokens) - 1):\n",
        "    # Adjust the index\n",
        "    k = k + 1\n",
        "    # Loop trhough tags for the current word\n",
        "    for j in range(len(tags)):\n",
        "      # Variables to hold best score for this tag and the node it came from\n",
        "      max_tag_score = 0\n",
        "      best_prev_tag = 0\n",
        "      # Loop through the previous word tags\n",
        "      for i in range(len(tags)):\n",
        "        #Calculate the jth tag score for the current word with the previous word's ith tag\n",
        "        current_score = sentence_scores[i, k - 1] + transitions[i, j] + word_tag_probabilities[k][j]\n",
        "        # Update the max score and previous node if this score is better\n",
        "        if current_score > max_tag_score:\n",
        "          max_tag_score = current_score\n",
        "          best_prev_tag = i\n",
        "      \n",
        "      #filling jth tag at word k with best possible path from the kth-1 word  with ith tag\n",
        "      sentence_scores[j][k] = max_tag_score   \n",
        "      #Saving the ith tag as the node this score came from \n",
        "      sentence_tags[j][k] = best_prev_tag\n",
        "\n",
        "  \n",
        "  # List to hold the best sentence tags\n",
        "  final_sentence_tags = [0 for i in range(len(tokens))]\n",
        "  # Variables to hold the last token score and the tag\n",
        "  last_tag = 0\n",
        "  last_tag_score = sentence_scores[last_tag][len(tokens) - 1]\n",
        "  # Find the best tag for the last token\n",
        "  for i in range(len(tags)):\n",
        "    if sentence_scores[i][len(tokens) - 1] > last_tag_score:\n",
        "      last_tag_score = sentence_scores[i][len(tokens) - 1]\n",
        "      last_tag = i\n",
        "  # Set the last token's tag as the one with the highest score\n",
        "  final_sentence_tags[len(tokens) - 1] = last_tag \n",
        "\n",
        "  # Backtrack trhough the tags matrix from the second to last word to the first word\n",
        "  for i in range(len(tokens) - 1):\n",
        "    # Fix the index value for backtracking\n",
        "    index = len(tokens) - i - 2\n",
        "    # This word's tag is the one marked in tags matrix for the word that comes after it (reading the sentence left to right)\n",
        "    final_sentence_tags[index] = sentence_tags[int(last_tag)][index + 1]\n",
        "    # Update the last tag to the tag we just set, to be used for the next word\n",
        "    last_tag = sentence_tags[int(last_tag)][index + 1]\n",
        "\n",
        "  # Return the togs, correct tags, and viterbi decodert tags in a list\n",
        "  fields = zip(s['tokens'], s['tags'], [model.vocab.get_token_from_index(i, 'labels') for i in final_sentence_tags])\n",
        "  return list(fields)\n",
        "\n",
        "# Using viterbi decoder, get the output\n",
        "viterbi_output = [viterbi_tag_sentence(i) for i in validation_dataset]\n",
        "\n",
        "\n",
        "#Calculate violations and statistics of viterbit output and baseline output\n",
        "#Both are outputs are from the validation dataset\n",
        "viterbi_output_violations = violations(viterbi_output)\n",
        "baseline_output_violations = violations(baseline_output)\n",
        "\n",
        "viterbi_output_stats = span_stats(viterbi_output)\n",
        "baseline_output_stats = span_stats(baseline_output)\n",
        "\n",
        "\n",
        "print(\"Baseline\")\n",
        "print(\"Violations: \" + str(baseline_output_violations))\n",
        "print(baseline_output_stats)\n",
        "print(\"\\n\")\n",
        "print(\"Viterbi\")\n",
        "print(\"Violations: \" + str(viterbi_output_violations))\n",
        "print(viterbi_output_stats)\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline\n",
            "Violations: 29\n",
            "{'precision': 0.6111111111111112, 'recall': 0.38823529411764707, 'f1': 0.47482014388489213}\n",
            "\n",
            "\n",
            "Viterbi\n",
            "Violations: 0\n",
            "{'precision': 0.618421052631579, 'recall': 0.5529411764705883, 'f1': 0.5838509316770186}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}