{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NULabTMN/ps2-Connor-Frazier/blob/development/LanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWB9udaKesP",
        "colab_type": "text"
      },
      "source": [
        "Your task is to train *character-level* language models. \n",
        "You will train unigram, bigram, and trigram character level models on a collection of books from Project Gutenberg. You will then use these trained English language models to distinguish English documents from Brazilian Portuguese documents in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHFJmuftHJld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import httpimport\n",
        "import math\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from decimal import Decimal\n",
        "import sys\n",
        "\n",
        "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from lm_helper import get_train_data, get_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U0UCuyHQkai",
        "colab_type": "text"
      },
      "source": [
        "This code loads the training and test data. Each dataset is a list of books. Each book contains a list of sentences, and each sentence contains a list of words. For building a character language model, you should join the words of a sentence together with a space character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0pfuiEChTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the train and test data\n",
        "train = get_train_data()\n",
        "test, test_files = get_test_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WAO9VjFLArq",
        "colab_type": "text"
      },
      "source": [
        "## 1.1\n",
        "Collect statistics on the unigram, bigram, and trigram character counts.\n",
        "\n",
        "If your machine takes a long time to perform this computation, you may save these counts to files in your github repository and load them on request. This is not necessary, however."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4VOoiSIoUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Create a held out set with four of the training books\n",
        "# Split training 80% = 14\n",
        "# Split held-out 20% = 4 \n",
        "# Used for finding lambdas for  linear interpolation smoothing\n",
        "# Uncomment below to set up lambda training\n",
        "held_out = [train.pop(3), train.pop(6), train.pop(9), train.pop(12)]\n",
        "\n",
        "\n",
        "\"\"\"Find counts for all unigrams, bigrams, trigrams\"\"\"\n",
        "# Create data structure to save counts\n",
        "data_dict = {}\n",
        "data_dict['unigrams'] = {}\n",
        "data_dict['bigrams'] = {}\n",
        "data_dict['trigrams'] = {}\n",
        "\n",
        "# Turn traing data into one string with all words and punctuation\n",
        "sentence_string = \"\"\n",
        "for example in train:\n",
        "  for sentence in example:\n",
        "    sentence_string += \"#\" # Add sentence begin character\n",
        "    for word in sentence:\n",
        "        sentence_string += word + \" \"\n",
        "    sentence_string += \"#\" # Add sentence end character\n",
        "\n",
        "# Unigram counts\n",
        "for i in range(len(sentence_string)):\n",
        "  if sentence_string[i: i+1] not in data_dict['unigrams']:\n",
        "    data_dict['unigrams'][sentence_string[i: i+1]] = 1\n",
        "  else:\n",
        "    data_dict['unigrams'][sentence_string[i: i+1]] += 1\n",
        "# Bigram counts\n",
        "for i in range(len(sentence_string) - 1):\n",
        "  if sentence_string[i: i+2] not in data_dict['bigrams']:\n",
        "    data_dict['bigrams'][sentence_string[i: i+2]] = 1\n",
        "  else:\n",
        "    data_dict['bigrams'][sentence_string[i: i+2]] += 1\n",
        "# Trigram counts\n",
        "for i in range(len(sentence_string) -2):\n",
        "  if sentence_string[i: i+3] not in data_dict['trigrams']:\n",
        "    data_dict['trigrams'][sentence_string[i: i+3]] = 1\n",
        "  else:\n",
        "    data_dict['trigrams'][sentence_string[i: i+3]] += 1      \n",
        "\n",
        "\n",
        "\"\"\"Find probabilities for all unigrams, bigrams, trigrams\"\"\"\n",
        "unigram_probabilities = {}\n",
        "bigram_probabilities = {}\n",
        "trigram_probabilities = {}\n",
        "\n",
        "# Unigrams probabilities\n",
        "unigram_total_count = 0\n",
        "for key, value in data_dict['unigrams'].items():\n",
        "  unigram_total_count += value\n",
        "\n",
        "for key, value in data_dict['unigrams'].items():\n",
        "  unigram_probabilities[key] = (value/unigram_total_count)\n",
        "\n",
        "# Bigrams probabilities\n",
        "for key, value in data_dict['bigrams'].items():\n",
        "  bigram_probabilities[key] = (value/data_dict['unigrams'][key[0]])\n",
        "  \n",
        "# Trigrams probabilities\n",
        "for key, value in data_dict['trigrams'].items():\n",
        "  trigram_probabilities[key] = (value/data_dict['bigrams'][key[0:2]])       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3mnaIvQnhI",
        "colab_type": "text"
      },
      "source": [
        "## 1.2\n",
        "Calculate the perplexity for each document in the test set using linear interpolation smoothing method. For determining λs for linear interpolation, you can divide the training data into a new training set (80%) and a held-out set (20%), then using grid search method:\n",
        "Choose ~10 values of λ to test using grid search on held-out data.\n",
        "\n",
        "Some documents in the test set are in Brazilian Portuguese. Identify them as follows: \n",
        "  - Sort by perplexity and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names (from `test_files`) and perplexities of the documents above the threshold\n",
        "\n",
        "    ```\n",
        "        file name, score\n",
        "        file name, score\n",
        "        . . .\n",
        "        file name, score\n",
        "    ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them as being correctly or incorrectly labeled as Portuguese.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQF4HhQGOZD8",
        "colab_type": "code",
        "outputId": "a99c0009-224f-4062-bd76-df7237cc4158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Your code here\n",
        "\n",
        "#Uncomment commented block below to run lambda training\n",
        "# \"\"\"Find the best lambda values on the held out set\"\"\"\n",
        "# # Taken from https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html\n",
        "# # Set up grid search\n",
        "# param_grid = {'L1': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'L2': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'L3': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}\n",
        "# parameters = list(ParameterGrid(param_grid))\n",
        "\n",
        "# # Remove all parameter sets that do not sum to 1\n",
        "# valid_parameters = []\n",
        "# for iteration in parameters:\n",
        "#   iteration_sum = 0\n",
        "#   for key, value in iteration.items():\n",
        "#     iteration_sum += value\n",
        "#   valid = abs(iteration_sum - float(1)) < 0.0000000000001\n",
        "#   if valid == True:\n",
        "#     valid_parameters.append(iteration)\n",
        "\n",
        "# \"\"\"Find the best parameter set\"\"\"\n",
        "\n",
        "# # Variables to hold best perplexity and parameter set\n",
        "# min_perplexity = sys.maxsize\n",
        "# best_parameters = valid_parameters[0]\n",
        "\n",
        "# # Loop through all valid parameter sets\n",
        "# for parameter_set in valid_parameters:\n",
        "#   #Set lambda values for this iteration\n",
        "#   l1 = parameter_set[\"L1\"]\n",
        "#   l2 = parameter_set[\"L2\"]\n",
        "#   l3 = parameter_set[\"L3\"]\n",
        "\n",
        "#   # Set entropy, and token count variables for this iteration\n",
        "#   token_count = 0\n",
        "#   held_out_entropy = 0\n",
        "\n",
        "#   # Create string containing all words in the held out set\n",
        "#   sentence_string = \"\"\n",
        "#   for example in held_out:\n",
        "#     for sentence in example:\n",
        "#       sentence_string += \"#\" # Add sentence begin character\n",
        "#       for word in sentence:\n",
        "#         sentence_string += word + \" \"\n",
        "#       sentence_string += \"#\" # Add sentence end character  \n",
        "\n",
        "#   # Loop through all of the trigrams in the validation string\n",
        "#   for j in range(len(sentence_string) -2):\n",
        "#     trigram = sentence_string[j: j+3]\n",
        "#     token_count += 1\n",
        "\n",
        "#     # Better solution for unk tokens?? Simple solution taken from https://www.usna.edu/Users/cs/nchamber/courses/nlp/f13/slides/set3-LMs.pdf\n",
        "    \n",
        "#     # Calculate the trigram probability\n",
        "#     trigram_probability = 0\n",
        "#     # If the trigram probability is know add it, else add the replacement probability\n",
        "#     if trigram in trigram_probabilities:\n",
        "#       trigram_probability += (l3 * trigram_probabilities[trigram])\n",
        "#     else:\n",
        "#       trigram_probability += (l3 * (1/len(trigram_probabilities)))\n",
        "\n",
        "#     # If the bigram probability is know add it, else add the replacement probability\n",
        "#     if trigram[0:2] in bigram_probabilities:\n",
        "#       trigram_probability += (l2 * bigram_probabilities[trigram[0:2]])\n",
        "#     else:\n",
        "#       trigram_probability += (l2 * (1/len(bigram_probabilities)))\n",
        "\n",
        "#     # If the unigram probability is know add it, else add the replacement probability\n",
        "#     if trigram[0] in unigram_probabilities:\n",
        "#       trigram_probability += (l1 * unigram_probabilities[trigram[0]])\n",
        "#     else:\n",
        "#       trigram_probability += (l1 * (1/len(unigram_probabilities)))  \n",
        "\n",
        "#     # Add the log probability to the held out total entropy\n",
        "#     held_out_entropy += math.log2(trigram_probability)\n",
        "    \n",
        "#   # Calculate the held out set perplexity\n",
        "#   held_out_set_perplexity = 2**((held_out_entropy * -1)/token_count)\n",
        "#   # print(held_out_set_perplexity)\n",
        "\n",
        "#   # Update the best perplexity and parameter set if it is better\n",
        "#   if held_out_set_perplexity < min_perplexity:\n",
        "#     min_perplexity = held_out_set_perplexity\n",
        "#     best_parameters = parameter_set\n",
        "\n",
        "# print(min_perplexity)\n",
        "# print(best_parameters)      \n",
        "# {'L1': 0.1, 'L2': 0.3, 'L3': 0.6}\n",
        "\n",
        "#Comment below block if running lambda training\n",
        "\"\"\"Find the perplexities of the test documents\"\"\"\n",
        "# Set the lambdas\n",
        "l1 = 0.1\n",
        "l2 = 0.3\n",
        "l3 = 0.6\n",
        "# Data structure to hold perplexities of each test document\n",
        "test_perplexities = {}\n",
        "\n",
        "# Loop through all of the doucments in the test set\n",
        "for example in test:\n",
        "# Set entropy, and token count variables for this iteration\n",
        "  entropy = 0\n",
        "  token_count = 0\n",
        "# Create string containing all words in the document\n",
        "  sentence_string = \"\"\n",
        "  for sentence in example:\n",
        "    sentence_string += \"#\" # Add sentence begin character\n",
        "    for word in sentence:\n",
        "      sentence_string += word + \" \"\n",
        "    sentence_string += \"#\" # Add sentence end character\n",
        "\n",
        "  # Loop through all of the trigrams in the document string\n",
        "  for j in range(len(sentence_string) -2):\n",
        "    trigram = sentence_string[j: j+3]\n",
        "    token_count += 1\n",
        "          \n",
        "    # Calculate the trigram probability\n",
        "    trigram_probability = 0\n",
        "\n",
        "    # If the trigram probability is known add it, else add the replacement probability\n",
        "    if trigram in trigram_probabilities:\n",
        "      trigram_probability += (l3 * trigram_probabilities[trigram])\n",
        "    else:\n",
        "      trigram_probability += (l3 * (1/len(trigram_probabilities)))\n",
        "\n",
        "    # If the bigram probability is known add it, else add the replacement probability\n",
        "    if trigram[0:2] in bigram_probabilities:\n",
        "      trigram_probability += (l2 * bigram_probabilities[trigram[0:2]])\n",
        "    else:\n",
        "      trigram_probability += (l2 * (1/len(bigram_probabilities)))\n",
        "\n",
        "    # If the unigram probability is known add it, else add the replacement probability\n",
        "    if trigram[0] in unigram_probabilities:\n",
        "      trigram_probability += (l1 * unigram_probabilities[trigram[0]])\n",
        "    else:\n",
        "      trigram_probability += (l1 * (1/len(unigram_probabilities)))\n",
        "\n",
        "    # Add the log probability to the document total entropy\n",
        "    entropy += math.log2(trigram_probability)\n",
        "\n",
        "  #Calculate the test file perplexity\n",
        "  test_file_perplexity = 2**(-entropy/token_count)\n",
        "  # print(test_file_perplexity)\n",
        "\n",
        "  # Save the test file perplexity\n",
        "  test_perplexities[test_files[test.index(example)]] = test_file_perplexity\n",
        "\n",
        "# Setting threshold at 10 based off obeservations\n",
        "# Loop trhough the test file perplexities, and print the ones above the threshold\n",
        "for key, value in test_perplexities.items():\n",
        "  if value > 10:\n",
        "    print(\"File Name: %s, Perplexity: %f\" % (key, value))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.149289978007447\n",
            "{'L1': 0, 'L2': 0.3, 'L3': 0.7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQl2u_giVW5e",
        "colab_type": "text"
      },
      "source": [
        "## 1.3\n",
        "Build a trigram language model with add-λ smoothing (use λ = 0.1).\n",
        "\n",
        "Sort the test documents by perplexity and perform a check for Brazilian Portuguese documents as above:\n",
        "\n",
        "  - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names and perplexities of the documents above the threshold\n",
        "\n",
        "  ```\n",
        "      file name, score\n",
        "      file name, score\n",
        "      . . .\n",
        "      file name, score\n",
        "  ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them for correctness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGUTEk8QUehL",
        "colab_type": "code",
        "outputId": "d9fb3347-b25a-4812-a094-ae7c9a209eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\"\"\"Find the perplexities of the test documents\"\"\"\n",
        "\n",
        "# Set the lambda\n",
        "l3 = 0.1\n",
        "\n",
        "# Data structure to hold perplexities of each test document\n",
        "test_perplexities = {}\n",
        "\n",
        "# Loop through all of the doucments in the test set\n",
        "for example in test:\n",
        "  # Set entropy, and token count variables for this iteration\n",
        "  entropy = 0\n",
        "  token_count = 0\n",
        "  # Create string containing all words in the document\n",
        "  sentence_string = \"\"\n",
        "  for sentence in example:\n",
        "    sentence_string += \"#\" # Add sentence begin character\n",
        "    for word in sentence:\n",
        "      sentence_string += word + \" \"\n",
        "    sentence_string += \"#\" # Add sentence end character\n",
        "\n",
        "  # Loop through all of the trigrams in the document string\n",
        "  for j in range(len(sentence_string) -2):\n",
        "    trigram = sentence_string[j: j+3]\n",
        "    token_count += 1\n",
        "              \n",
        "    # Calculate the trigram probability\n",
        "    trigram_probability = 0\n",
        "\n",
        "    # If the trigram probability is known add it, else add the replacement probability\n",
        "    if trigram in trigram_probabilities:\n",
        "      trigram_probability += (l3 * trigram_probabilities[trigram])\n",
        "    else:\n",
        "      trigram_probability += (l3 * (1/len(trigram_probabilities)))\n",
        "\n",
        "    # Add the log probability to the document total entropy\n",
        "    entropy += math.log2(trigram_probability)\n",
        "\n",
        "  \n",
        "  #Calculate the test file perplexity\n",
        "  test_file_perplexity = 2**(-entropy/token_count)\n",
        "  # print(test_file_perplexity)\n",
        "\n",
        "  # Save the test file perplexity\n",
        "  test_perplexities[test_files[test.index(example)]] = test_file_perplexity\n",
        "\n",
        "\n",
        "# Setting threshold at 200 based off obeservations\n",
        "# Loop trhough the test file perplexities, and print the ones above the threshold\n",
        "for key, value in test_perplexities.items():\n",
        "  if value > 200:\n",
        "    print(\"File Name: %s, Perplexity: %f\" % (key, value))\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: br94fe1.txt, Perplexity: 393.809458\n",
            "File Name: ag94ju07.txt, Perplexity: 404.608064\n",
            "File Name: br94de01.txt, Perplexity: 374.623320\n",
            "File Name: ag94ag02.txt, Perplexity: 397.831267\n",
            "File Name: ag94se06.txt, Perplexity: 421.017679\n",
            "File Name: ag94mr1.txt, Perplexity: 392.869303\n",
            "File Name: ag94ab12.txt, Perplexity: 397.907121\n",
            "File Name: ag94ou04.txt, Perplexity: 385.807471\n",
            "File Name: ag94fe1.txt, Perplexity: 362.680538\n",
            "File Name: ag94ma03.txt, Perplexity: 411.135439\n",
            "File Name: br94ab02.txt, Perplexity: 368.537224\n",
            "File Name: ag94de06.txt, Perplexity: 391.560078\n",
            "File Name: br94ju01.txt, Perplexity: 375.132363\n",
            "File Name: br94ma01.txt, Perplexity: 395.006771\n",
            "File Name: br94jl01.txt, Perplexity: 377.504859\n",
            "File Name: br94ag01.txt, Perplexity: 400.657841\n",
            "File Name: br94ja04.txt, Perplexity: 385.280533\n",
            "File Name: ag94jl12.txt, Perplexity: 392.513347\n",
            "File Name: ag94no01.txt, Perplexity: 391.713588\n",
            "File Name: ag94ja11.txt, Perplexity: 371.802422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhXTB5TXR25",
        "colab_type": "text"
      },
      "source": [
        "## 1.4\n",
        "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFq1ECgDI6QG",
        "colab_type": "text"
      },
      "source": [
        "Run instrunctions:\n",
        "\n",
        "Currently running all sections will include:\n",
        "\n",
        "1.1:\n",
        "\n",
        "Collect all counts of unigrams, bigrams, and trigrams.\n",
        "Calculate empirical probabilities for all unigrams, bigrams, and trigrams.\n",
        "\n",
        "1.2:\n",
        "\n",
        "Calculate the perplexity of each test document using linear interpolation smoothing and report the files that are above threshold.\n",
        "\n",
        "1.3:\n",
        "\n",
        "Calculate the perplexity of each test document using the trigram model with lambda smoothing and report the files that are above threshold.\n",
        "\n",
        "If you want to see how the lamdas are determined, first uncomment the heldout set list creation at the top of section 1.1. Second, uncomment the first large commented code block at the top of 1.2, and comment out the large code block in the second half of 1.2. To run the code in order to determine the test files that are portuguese, just reverse these steps. One thing to note, the best parameters from the validation set were {'L1': 0, 'L2': 0.3, 'L3': 0.7} where L1 corresponds to unigrams, L2 to bigrams, and L3 to trigrams. However {'L1': 0.1, 'L2': 0.3, 'L3': 0.6} was chosen to be used so that unigram probabilities would be included.\n",
        "\n",
        "\n",
        "\n",
        "Observations:\n",
        "\n",
        "Both methods were able to show the perplexity difference between the english and prtuguese files but with different levels of perplexity. The linear interpolation smoothing method reported the english documents perplexities as less than 10 and the portuguese documents between 13 and 15 creating a clear difference between the two. The lamda smoothing trigram model reported much higher perplexities where the english documents were around 180 and the portuguese documents were around 400 which also was another clear difference between the documents. Both methods did not perform as well when attempting to use an UNK token probability for unrecognized tokens. The perplexities were overall much smaller barely separated enough to tell the difference in documents. Therefore a uniform distribution was used to provide a probability for unrecognized tokens during calcuations.\n",
        "\n",
        "Based on these observations, I have found that the linear interpolation smoothing method works much better for character level language modeling. It has the ability to use all of the information in the trigram by recognizing smaller amounts of characters in order to better identify language it has seen before.\n",
        "\n",
        "\n",
        "Linear Interpolation Smoothing:\n",
        "\n",
        "Pros:\n",
        "\n",
        "Ability to backoff to recognize bigrams and unigrams \n",
        "\n",
        "Ability to use all knowledge contained in a trigram including a trigram's subsets of characters\n",
        "\n",
        "Lower Perplexities from using the models complete knowledge\n",
        "\n",
        "Cons:\n",
        "\n",
        "More computation required\n",
        "\n",
        "More information gathering\n",
        "\n",
        "Requires finding Lambda weights\n",
        "\n",
        "\n",
        "Add Lambda Smoothing:\n",
        "\n",
        "Pros:\n",
        "\n",
        "Simple approach that still accomplishes the task\n",
        "\n",
        "Less Computation\n",
        "\n",
        "Cons:\n",
        "\n",
        "Does not use all available information\n",
        "\n",
        "Misses oppurtunity to recognize smaller bits of information\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}